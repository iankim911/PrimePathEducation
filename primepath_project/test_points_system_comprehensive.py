#!/usr/bin/env python3
"""
COMPREHENSIVE POINTS SYSTEM TESTING AND MONITORING SCRIPT

This script provides comprehensive testing and monitoring for the points editing system.
It verifies all aspects of the implementation including:

1. PointsService functionality
2. API endpoint behavior
3. Database integrity
4. Session recalculation accuracy
5. Frontend integration
6. Performance benchmarks

Usage:
    python test_points_system_comprehensive.py
"""

import os
import sys
import json
import time
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional

# Setup Django environment
sys.path.append('/Users/ian/Desktop/VIBECODE/PrimePath/primepath_project')
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'primepath_project.settings_sqlite')

import django
django.setup()

from django.test import TestCase, Client
from django.contrib.auth.models import User
from django.db import transaction
from django.core.management import call_command

from placement_test.models import Question, Exam, StudentSession, StudentAnswer
from placement_test.services.points_service import PointsService
from placement_test.services.grading_service import GradingService

# Configure logging for comprehensive monitoring
logging.basicConfig(
    level=logging.INFO,
    format='[%(levelname)s] %(asctime)s %(name)s: %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('points_system_test_results.log')
    ]
)

logger = logging.getLogger('PointsSystemTester')

class PointsSystemTester:
    """
    Comprehensive testing suite for the points editing system.
    """
    
    def __init__(self):
        self.client = Client()
        self.test_results = {
            'timestamp': datetime.now().isoformat(),
            'tests_run': 0,
            'tests_passed': 0,
            'tests_failed': 0,
            'failures': [],
            'performance_metrics': {},
            'coverage_report': {}
        }
        
        # Create test user if needed
        self.test_user = None
        try:
            self.test_user, created = User.objects.get_or_create(
                username='points_test_user',
                defaults={
                    'email': 'test@example.com',
                    'is_staff': True,
                    'is_superuser': True
                }
            )
            if created:
                self.test_user.set_password('testpass123')
                self.test_user.save()
                logger.info("Created test user for points system testing")
        except Exception as e:
            logger.error(f"Failed to create test user: {e}")
    
    def run_test(self, test_name: str, test_func, *args, **kwargs):
        """Run a single test with error handling and timing."""
        logger.info(f"üß™ Running test: {test_name}")
        start_time = time.time()
        
        try:
            result = test_func(*args, **kwargs)
            duration = time.time() - start_time
            
            self.test_results['tests_run'] += 1
            self.test_results['tests_passed'] += 1
            self.test_results['performance_metrics'][test_name] = {
                'duration_seconds': duration,
                'status': 'PASSED',
                'result': result
            }
            
            logger.info(f"‚úÖ {test_name} PASSED ({duration:.3f}s)")
            return True, result
            
        except Exception as e:
            duration = time.time() - start_time
            error_msg = str(e)
            
            self.test_results['tests_run'] += 1
            self.test_results['tests_failed'] += 1
            self.test_results['failures'].append({
                'test_name': test_name,
                'error': error_msg,
                'duration': duration
            })
            self.test_results['performance_metrics'][test_name] = {
                'duration_seconds': duration,
                'status': 'FAILED',
                'error': error_msg
            }
            
            logger.error(f"‚ùå {test_name} FAILED ({duration:.3f}s): {error_msg}")
            return False, error_msg
    
    def test_points_service_validation(self):
        """Test PointsService validation logic."""
        logger.info("Testing PointsService validation...")
        
        # Test valid values
        valid_cases = [1, 2, 5, 10, "5", "10"]
        for value in valid_cases:
            is_valid, error, validated = PointsService.validate_points_value(value)
            assert is_valid, f"Expected {value} to be valid, got error: {error}"
            assert isinstance(validated, int), f"Expected integer result, got {type(validated)}"
        
        # Test invalid values
        invalid_cases = [0, -1, 11, 15, "abc", "", None, 5.5, "5.5"]
        for value in invalid_cases:
            is_valid, error, validated = PointsService.validate_points_value(value)
            assert not is_valid, f"Expected {value} to be invalid, but it passed validation"
            assert error, f"Expected error message for {value}"
        
        return "Validation tests passed"
    
    def test_points_service_update(self):
        """Test PointsService update functionality."""
        logger.info("Testing PointsService update...")
        
        # Get a test question
        question = Question.objects.first()
        if not question:
            raise Exception("No questions found for testing")
        
        original_points = question.points
        new_points = original_points + 1 if original_points < 10 else original_points - 1
        
        # Test update
        result = PointsService.update_question_points(
            question_id=question.id,
            new_points=new_points,
            user_id=self.test_user.id if self.test_user else None,
            recalculate_sessions=False  # Skip recalculation for speed
        )
        
        assert result['success'], f"Update failed: {result.get('error')}"
        assert result['new_points'] == new_points
        assert result['old_points'] == original_points
        
        # Verify database was updated
        question.refresh_from_db()
        assert question.points == new_points
        
        # Restore original value
        question.points = original_points
        question.save()
        
        return f"Successfully updated question {question.id}: {original_points} -> {new_points} -> {original_points}"
    
    def test_api_endpoints(self):
        """Test API endpoint functionality."""
        logger.info("Testing API endpoints...")
        
        # Login test user
        if self.test_user:
            self.client.force_login(self.test_user)
        
        # Test update question endpoint
        question = Question.objects.first()
        if not question:
            raise Exception("No questions found for API testing")
        
        original_points = question.points
        new_points = original_points + 1 if original_points < 10 else original_points - 1
        
        response = self.client.post(
            f'/api/PlacementTest/questions/{question.id}/update/',
            data={'points': new_points},
            HTTP_X_REQUESTED_WITH='XMLHttpRequest'
        )
        
        assert response.status_code == 200, f"Expected 200, got {response.status_code}"
        
        data = response.json()
        assert data['success'], f"API call failed: {data.get('error')}"
        
        # Verify update
        question.refresh_from_db()
        assert question.points == new_points
        
        # Test analytics endpoint
        response = self.client.get(
            f'/api/PlacementTest/points/analytics/',
            HTTP_X_REQUESTED_WITH='XMLHttpRequest'
        )
        
        assert response.status_code == 200
        analytics_data = response.json()
        assert analytics_data['success']
        
        # Test impact preview endpoint
        response = self.client.get(
            f'/api/PlacementTest/questions/{question.id}/points/impact-preview/',
            HTTP_X_REQUESTED_WITH='XMLHttpRequest'
        )
        
        assert response.status_code == 200
        impact_data = response.json()
        assert impact_data['success']
        
        # Restore original value
        question.points = original_points
        question.save()
        
        return "API endpoints functioning correctly"
    
    def test_grading_integration(self):
        """Test grading system integration with points."""
        logger.info("Testing grading system integration...")
        
        # Find a question with existing answers
        question_with_answers = Question.objects.filter(
            answers__isnull=False
        ).first()
        
        if not question_with_answers:
            logger.warning("No questions with answers found, creating test data...")
            # Create minimal test data
            exam = Exam.objects.first()
            if not exam:
                raise Exception("No exams found for testing")
                
            session = StudentSession.objects.create(
                exam=exam,
                student_name="Test Student",
                student_email="test@example.com",
                phone_number="1234567890"
            )
            
            question = exam.questions.first()
            if not question:
                raise Exception("No questions in exam for testing")
            
            answer = StudentAnswer.objects.create(
                session=session,
                question=question,
                answer="A",  # Assuming correct answer
                is_correct=True,
                points_earned=question.points
            )
            
            question_with_answers = question
        
        # Test grading with different point values
        original_points = question_with_answers.points
        test_points_values = [1, 3, 5]
        
        for points_value in test_points_values:
            question_with_answers.points = points_value
            question_with_answers.save()
            
            # Get an answer for this question
            sample_answer = StudentAnswer.objects.filter(
                question=question_with_answers
            ).first()
            
            if sample_answer:
                # Test auto-grading
                grade_result = GradingService.auto_grade_answer(sample_answer)
                
                if grade_result['is_correct']:
                    assert grade_result['points_earned'] == points_value, \
                        f"Expected {points_value} points, got {grade_result['points_earned']}"
        
        # Restore original value
        question_with_answers.points = original_points
        question_with_answers.save()
        
        return "Grading integration working correctly"
    
    def test_analytics_generation(self):
        """Test analytics generation functionality."""
        logger.info("Testing analytics generation...")
        
        # Test global analytics
        global_analytics = PointsService.get_points_analytics()
        assert global_analytics['success'], f"Analytics failed: {global_analytics.get('error')}"
        
        # Verify analytics structure
        required_keys = ['overview', 'points_distribution', 'by_question_type', 'recommendations']
        for key in required_keys:
            assert key in global_analytics, f"Missing key in analytics: {key}"
        
        # Test exam-specific analytics
        exam = Exam.objects.first()
        if exam:
            exam_analytics = PointsService.get_points_analytics(exam_id=exam.id)
            assert exam_analytics['success'], f"Exam analytics failed: {exam_analytics.get('error')}"
        
        return "Analytics generation working correctly"
    
    def test_performance_benchmarks(self):
        """Test performance of points operations."""
        logger.info("Testing performance benchmarks...")
        
        # Test single update performance
        question = Question.objects.first()
        if not question:
            raise Exception("No questions for performance testing")
        
        start_time = time.time()
        
        result = PointsService.update_question_points(
            question_id=question.id,
            new_points=question.points,  # No actual change
            recalculate_sessions=False
        )
        
        single_update_time = time.time() - start_time
        assert single_update_time < 1.0, f"Single update took too long: {single_update_time:.3f}s"
        
        # Test analytics generation performance
        start_time = time.time()
        analytics = PointsService.get_points_analytics()
        analytics_time = time.time() - start_time
        assert analytics_time < 2.0, f"Analytics generation took too long: {analytics_time:.3f}s"
        
        return {
            'single_update_time': single_update_time,
            'analytics_time': analytics_time
        }
    
    def test_database_integrity(self):
        """Test database integrity and constraints."""
        logger.info("Testing database integrity...")
        
        # Test that points field constraints are enforced
        question = Question.objects.first()
        if not question:
            raise Exception("No questions for integrity testing")
        
        original_points = question.points
        
        # Test minimum constraint (should be handled by PointsService validation)
        try:
            result = PointsService.update_question_points(
                question_id=question.id,
                new_points=0  # Invalid value
            )
            assert not result['success'], "Expected validation to fail for points=0"
        except Exception as e:
            pass  # Expected to fail
        
        # Verify question wasn't changed
        question.refresh_from_db()
        assert question.points == original_points
        
        return "Database integrity preserved"
    
    def generate_coverage_report(self):
        """Generate coverage report for the points system."""
        logger.info("Generating coverage report...")
        
        # Check which components have been tested
        coverage = {
            'points_service': {
                'validation': True,
                'update_single': True,
                'update_bulk': False,  # Not tested yet
                'analytics': True,
                'impact_preview': False  # Not directly tested
            },
            'api_endpoints': {
                'update_question': True,
                'get_analytics': True,
                'get_impact_preview': True
            },
            'grading_integration': {
                'auto_grade_answer': True,
                'grade_session': False,  # Not fully tested
                'points_calculation': True
            },
            'frontend_integration': {
                'javascript_validation': False,  # Can't test from backend
                'ui_updates': False,  # Can't test from backend
                'impact_preview': False  # Can't test from backend
            }
        }
        
        self.test_results['coverage_report'] = coverage
        return coverage
    
    def run_all_tests(self):
        """Run all tests and generate comprehensive report."""
        logger.info("üöÄ Starting comprehensive points system testing...")
        logger.info("=" * 80)
        
        # Run all test suites
        test_suites = [
            ("PointsService Validation", self.test_points_service_validation),
            ("PointsService Update", self.test_points_service_update),
            ("API Endpoints", self.test_api_endpoints),
            ("Grading Integration", self.test_grading_integration),
            ("Analytics Generation", self.test_analytics_generation),
            ("Performance Benchmarks", self.test_performance_benchmarks),
            ("Database Integrity", self.test_database_integrity)
        ]
        
        for test_name, test_func in test_suites:
            self.run_test(test_name, test_func)
        
        # Generate coverage report
        self.generate_coverage_report()
        
        # Print comprehensive results
        self.print_final_report()
        
        return self.test_results
    
    def print_final_report(self):
        """Print comprehensive test results."""
        results = self.test_results
        
        logger.info("=" * 80)
        logger.info("üìä COMPREHENSIVE POINTS SYSTEM TEST RESULTS")
        logger.info("=" * 80)
        
        # Overall summary
        total_tests = results['tests_run']
        passed_tests = results['tests_passed']
        failed_tests = results['tests_failed']
        success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0
        
        logger.info(f"Tests Run: {total_tests}")
        logger.info(f"Passed: {passed_tests} ‚úÖ")
        logger.info(f"Failed: {failed_tests} ‚ùå")
        logger.info(f"Success Rate: {success_rate:.1f}%")
        
        # Performance summary
        if results['performance_metrics']:
            logger.info("\n‚ö° PERFORMANCE METRICS:")
            for test_name, metrics in results['performance_metrics'].items():
                status_emoji = "‚úÖ" if metrics['status'] == 'PASSED' else "‚ùå"
                logger.info(f"  {status_emoji} {test_name}: {metrics['duration_seconds']:.3f}s")
        
        # Failures detail
        if results['failures']:
            logger.info("\n‚ùå FAILED TESTS:")
            for failure in results['failures']:
                logger.info(f"  ‚Ä¢ {failure['test_name']}: {failure['error']}")
        
        # Coverage summary
        coverage = results.get('coverage_report', {})
        if coverage:
            logger.info("\nüìã COVERAGE REPORT:")
            for component, tests in coverage.items():
                component_coverage = sum(tests.values()) / len(tests) * 100
                logger.info(f"  {component}: {component_coverage:.0f}% covered")
        
        # Save detailed results to file
        results_file = f"points_system_test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        logger.info(f"\nüíæ Detailed results saved to: {results_file}")
        logger.info("=" * 80)
        
        # Final verdict
        if failed_tests == 0:
            logger.info("üéâ ALL TESTS PASSED! Points system is functioning correctly.")
        else:
            logger.warning(f"‚ö†Ô∏è  {failed_tests} tests failed. Review failures above.")
        
        logger.info("=" * 80)


def main():
    """Main entry point for the testing script."""
    print("PRIMEPATH POINTS SYSTEM - COMPREHENSIVE TESTING & MONITORING")
    print("=" * 80)
    print(f"Starting comprehensive testing at {datetime.now()}")
    print("This may take a few minutes...")
    print()
    
    tester = PointsSystemTester()
    results = tester.run_all_tests()
    
    # Exit with appropriate code
    exit_code = 0 if results['tests_failed'] == 0 else 1
    print(f"\nTesting completed with exit code: {exit_code}")
    return exit_code


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)