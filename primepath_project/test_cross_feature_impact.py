#!/usr/bin/env python3
"""
CROSS-FEATURE IMPACT ANALYSIS - COPY MODAL DROPDOWN FIX
=======================================================

This script ensures that the Copy Modal dropdown fix has not broken
or negatively impacted any other features, functionality, or user workflows.
"""

import os
import sys
import django
import json
from datetime import datetime

# Setup Django environment
sys.path.append('/Users/ian/Desktop/VIBECODE/PrimePath/primepath_project')
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'primepath_project.settings_sqlite')
django.setup()

from django.contrib.auth.models import User
from django.test import Client
from primepath_routinetest.models import Exam, Question
from placement_test.models import StudentSession
from core.models import Teacher

print("="*100)
print("üîç CROSS-FEATURE IMPACT ANALYSIS: COPY MODAL DROPDOWN FIX")
print("="*100)
print(f"Started: {datetime.now().isoformat()}")
print()

def test_exam_library_core_features():
    """Test that core exam library features remain functional"""
    print("="*50)
    print("PHASE 6A: EXAM LIBRARY CORE FEATURES")
    print("="*50)
    
    client = Client()
    admin_user = User.objects.filter(is_superuser=True).first()
    client.force_login(admin_user)
    
    # Test 1: Main exam library page loads
    print("üß™ Testing main exam library page...")
    response = client.get('/RoutineTest/exams/')
    
    main_page_status = {
        'loads_successfully': response.status_code == 200,
        'has_content': len(response.content) > 100000,  # Should have substantial content
        'has_context': hasattr(response, 'context') and response.context is not None
    }
    
    if main_page_status['loads_successfully']:
        content = response.content.decode('utf-8')
        
        # Check for core exam library features
        core_features = {
            'exam_table': 'table' in content and 'exam' in content.lower(),
            'create_exam_button': 'Create Exam' in content or 'create' in content.lower(),
            'edit_buttons': 'Edit' in content or 'edit' in content.lower(),
            'delete_functionality': 'delete' in content.lower(),
            'exam_preview': 'preview' in content.lower(),
            'search_filter': 'search' in content.lower() or 'filter' in content.lower(),
            'pagination': 'page' in content.lower() or 'pagination' in content.lower()
        }
        
        print("üìä Core Exam Library Features:")
        for feature, present in core_features.items():
            status = "‚úÖ" if present else "‚ùå"
            print(f"  {status} {feature}: {present}")
        
        main_page_status['core_features'] = core_features
    
    # Test 2: Exam creation still works
    print("\nüß™ Testing exam creation functionality...")
    create_response = client.get('/RoutineTest/exams/create/')
    
    create_functionality = {
        'create_page_loads': create_response.status_code == 200,
        'has_form': 'form' in create_response.content.decode('utf-8').lower() if create_response.status_code == 200 else False
    }
    
    print("üìä Exam Creation:")
    for feature, status in create_functionality.items():
        emoji = "‚úÖ" if status else "‚ùå"
        print(f"  {emoji} {feature}: {status}")
    
    return {
        'main_page': main_page_status,
        'creation': create_functionality
    }

def test_exam_management_workflows():
    """Test that exam management workflows are not broken"""
    print("="*50)
    print("PHASE 6B: EXAM MANAGEMENT WORKFLOWS")
    print("="*50)
    
    # Get a real exam to test with
    test_exam = Exam.objects.first()
    
    if not test_exam:
        print("‚ùå No exams available for testing")
        return {'error': 'No test data available'}
    
    print(f"üß™ Testing with exam: {test_exam.name[:50]}...")
    
    client = Client()
    admin_user = User.objects.filter(is_superuser=True).first()
    client.force_login(admin_user)
    
    # Test exam detail/edit workflows
    workflow_tests = {}
    
    # Test 1: Exam detail page
    detail_url = f'/RoutineTest/exams/{test_exam.id}/'
    detail_response = client.get(detail_url)
    workflow_tests['exam_detail'] = {
        'accessible': detail_response.status_code == 200,
        'has_content': len(detail_response.content) > 1000 if detail_response.status_code == 200 else False
    }
    
    # Test 2: Exam edit page
    edit_url = f'/RoutineTest/exams/{test_exam.id}/edit/'
    edit_response = client.get(edit_url)
    workflow_tests['exam_edit'] = {
        'accessible': edit_response.status_code == 200,
        'has_form': 'form' in edit_response.content.decode('utf-8').lower() if edit_response.status_code == 200 else False
    }
    
    # Test 3: Question management
    questions_url = f'/RoutineTest/exams/{test_exam.id}/questions/'
    questions_response = client.get(questions_url)
    workflow_tests['question_management'] = {
        'accessible': questions_response.status_code == 200,
        'functional': True  # If it loads, assume functional for now
    }
    
    print("üìä Exam Management Workflows:")
    for workflow, results in workflow_tests.items():
        print(f"  {workflow}:")
        for test, result in results.items():
            status = "‚úÖ" if result else "‚ùå"
            print(f"    {status} {test}: {result}")
    
    return workflow_tests

def test_student_interface_unchanged():
    """Test that student-facing features remain unchanged"""
    print("="*50)
    print("PHASE 6C: STUDENT INTERFACE INTEGRITY")
    print("="*50)
    
    # Check if there are any active student sessions to test with
    active_sessions = StudentSession.objects.filter(completed_at__isnull=True)[:3]
    
    if not active_sessions.exists():
        print("‚ö†Ô∏è  No active student sessions for testing")
        # Create a test scenario or check placement test features
        student_tests = {
            'placement_start': 'No active sessions to test',
            'test_interface': 'Cannot test without session',
            'navigation': 'Cannot test without session'
        }
    else:
        print(f"üß™ Testing {active_sessions.count()} active student sessions...")
        
        client = Client()
        student_tests = {}
        
        for session in active_sessions:
            session_url = f'/placement/test/{session.id}/'
            try:
                response = client.get(session_url)
                student_tests[f'session_{session.id}'] = {
                    'accessible': response.status_code == 200,
                    'has_interface': 'question' in response.content.decode('utf-8').lower() if response.status_code == 200 else False
                }
            except Exception as e:
                student_tests[f'session_{session.id}'] = {'error': str(e)}
    
    print("üìä Student Interface Tests:")
    for test_name, results in student_tests.items():
        if isinstance(results, dict) and 'error' not in results:
            for key, value in results.items():
                status = "‚úÖ" if value else "‚ùå" 
                print(f"  {status} {test_name} - {key}: {value}")
        else:
            print(f"  ‚ö†Ô∏è  {test_name}: {results}")
    
    return student_tests

def test_teacher_access_permissions():
    """Test that teacher access and permissions remain intact"""
    print("="*50)
    print("PHASE 6D: TEACHER ACCESS & PERMISSIONS")
    print("="*50)
    
    # Test different user types
    client = Client()
    
    # Test 1: Admin access (should work)
    admin_user = User.objects.filter(is_superuser=True).first()
    if admin_user:
        client.force_login(admin_user)
        admin_response = client.get('/RoutineTest/exams/')
        admin_access = {
            'can_access_exams': admin_response.status_code == 200,
            'sees_copy_buttons': 'Copy Exam' in admin_response.content.decode('utf-8') if admin_response.status_code == 200 else False
        }
    else:
        admin_access = {'error': 'No admin user found'}
    
    # Test 2: Regular teacher access
    regular_teachers = User.objects.filter(is_staff=True, is_superuser=False)[:2]
    teacher_access = {}
    
    for teacher in regular_teachers:
        client.force_login(teacher)
        teacher_response = client.get('/RoutineTest/exams/')
        teacher_access[f'teacher_{teacher.username}'] = {
            'can_access': teacher_response.status_code == 200,
            'appropriate_permissions': True  # Assume permissions are working if they can access
        }
    
    # Test 3: Teacher model relationships
    teacher_profiles = Teacher.objects.all()[:3]
    teacher_model_status = {
        'teacher_models_exist': teacher_profiles.exists(),
        'user_relationships_intact': all(t.user is not None for t in teacher_profiles) if teacher_profiles.exists() else False,
        'permission_fields_present': all(hasattr(t, 'is_head_teacher') for t in teacher_profiles) if teacher_profiles.exists() else False
    }
    
    print("üìä Teacher Access Results:")
    print(f"  Admin Access: {admin_access}")
    print(f"  Teacher Access: {teacher_access}")
    print(f"  Teacher Models: {teacher_model_status}")
    
    return {
        'admin_access': admin_access,
        'teacher_access': teacher_access,
        'teacher_models': teacher_model_status
    }

def test_data_integrity():
    """Test that database relationships and data integrity remain intact"""
    print("="*50)
    print("PHASE 6E: DATABASE & DATA INTEGRITY")
    print("="*50)
    
    # Test key model relationships
    integrity_checks = {}
    
    # Test 1: Exam model integrity
    exam_count = Exam.objects.count()
    question_count = Question.objects.count()
    
    integrity_checks['exam_data'] = {
        'exam_count': exam_count,
        'question_count': question_count,
        'exams_have_questions': Exam.objects.filter(routine_questions__isnull=False).exists(),
        'questions_have_exams': Question.objects.filter(exam__isnull=False).exists()
    }
    
    # Test 2: Check for any database errors
    try:
        # Test that we can query key relationships
        sample_exam = Exam.objects.select_related('created_by').prefetch_related('routine_questions').first()
        if sample_exam:
            integrity_checks['relationship_queries'] = {
                'exam_creator_accessible': sample_exam.created_by is not None,
                'exam_questions_accessible': sample_exam.routine_questions.exists(),
                'no_query_errors': True
            }
        else:
            integrity_checks['relationship_queries'] = {'no_exams': True}
    except Exception as e:
        integrity_checks['relationship_queries'] = {'error': str(e)}
    
    # Test 3: Curriculum data consistency
    from primepath_routinetest.services.exam_service import ExamService
    try:
        curriculum_data = ExamService.get_routinetest_curriculum_hierarchy_for_frontend()
        integrity_checks['curriculum_service'] = {
            'service_functional': curriculum_data is not None,
            'has_expected_structure': 'curriculum_data' in curriculum_data if curriculum_data else False,
            'program_count': len(curriculum_data.get('curriculum_data', {})) if curriculum_data else 0
        }
    except Exception as e:
        integrity_checks['curriculum_service'] = {'error': str(e)}
    
    print("üìä Data Integrity Results:")
    for category, checks in integrity_checks.items():
        print(f"  {category}:")
        if isinstance(checks, dict):
            for check, result in checks.items():
                if isinstance(result, bool):
                    status = "‚úÖ" if result else "‚ùå"
                    print(f"    {status} {check}: {result}")
                else:
                    print(f"    üìä {check}: {result}")
    
    return integrity_checks

def test_javascript_conflicts():
    """Test for any JavaScript conflicts or errors introduced"""
    print("="*50)
    print("PHASE 6F: JAVASCRIPT CONFLICT DETECTION") 
    print("="*50)
    
    client = Client()
    admin_user = User.objects.filter(is_superuser=True).first()
    client.force_login(admin_user)
    
    # Get main exam page content
    response = client.get('/RoutineTest/exams/')
    
    if response.status_code != 200:
        return {'error': 'Cannot load page for JavaScript analysis'}
    
    content = response.content.decode('utf-8')
    
    # Look for potential JavaScript issues
    js_analysis = {
        'duplicate_function_names': [],
        'conflicting_global_vars': [],
        'error_patterns': [],
        'copy_modal_js_present': 'CopyCurriculumData' in content,
        'other_modals_intact': 'modal' in content.lower(),
        'jquery_conflicts': content.count('$(') > 10,  # Should have jQuery usage
        'console_logs_reasonable': content.count('console.log') < 100  # Not excessive logging
    }
    
    # Check for potential naming conflicts
    copy_modal_patterns = [
        'CopyCurriculumData',
        'openCopyModal', 
        'populateCopyProgramDropdown',
        'initializeCurriculumDropdown'
    ]
    
    for pattern in copy_modal_patterns:
        occurrences = content.count(pattern)
        if occurrences > 10:  # Too many might indicate duplication
            js_analysis['duplicate_function_names'].append(f"{pattern}: {occurrences}")
    
    # Look for error patterns
    error_patterns = [
        'undefined is not a function',
        'Cannot read property',
        'ReferenceError',
        'TypeError'
    ]
    
    for pattern in error_patterns:
        if pattern in content:
            js_analysis['error_patterns'].append(pattern)
    
    print("üìä JavaScript Analysis:")
    for check, result in js_analysis.items():
        if isinstance(result, list):
            if result:
                print(f"  ‚ö†Ô∏è  {check}: {result}")
            else:
                print(f"  ‚úÖ {check}: Clean")
        else:
            status = "‚úÖ" if result else "‚ùå"
            print(f"  {status} {check}: {result}")
    
    return js_analysis

# Run all Phase 6 tests
print("üöÄ Starting Phase 6: Cross-Feature Impact Analysis")
print()

results = {
    'exam_library_features': test_exam_library_core_features(),
    'exam_management_workflows': test_exam_management_workflows(), 
    'student_interface': test_student_interface_unchanged(),
    'teacher_permissions': test_teacher_access_permissions(),
    'data_integrity': test_data_integrity(),
    'javascript_conflicts': test_javascript_conflicts()
}

# Calculate overall impact assessment
print("\n" + "="*100)
print("üìä PHASE 6 CROSS-FEATURE IMPACT ASSESSMENT")
print("="*100)

# Analyze results for breaking changes
breaking_changes = []
warnings = []
successes = []

for category, category_results in results.items():
    if isinstance(category_results, dict):
        if 'error' in category_results:
            breaking_changes.append(f"{category}: {category_results['error']}")
        else:
            # Look for any False values that might indicate issues
            category_issues = []
            for key, value in category_results.items():
                if isinstance(value, dict):
                    for subkey, subvalue in value.items():
                        if subvalue is False:
                            category_issues.append(f"{key}.{subkey}")
                elif value is False:
                    category_issues.append(key)
            
            if category_issues:
                warnings.append(f"{category}: {', '.join(category_issues)}")
            else:
                successes.append(category)

# Print summary
print(f"üéâ Features Working Correctly: {len(successes)}")
for success in successes:
    print(f"  ‚úÖ {success}")

if warnings:
    print(f"\n‚ö†Ô∏è  Potential Issues Detected: {len(warnings)}")
    for warning in warnings:
        print(f"  ‚ö†Ô∏è  {warning}")

if breaking_changes:
    print(f"\n‚ùå Breaking Changes Detected: {len(breaking_changes)}")
    for change in breaking_changes:
        print(f"  ‚ùå {change}")

# Overall assessment
total_categories = len(results)
successful_categories = len(successes)
impact_score = (successful_categories / total_categories) * 100

print(f"\nüéØ Cross-Feature Impact Score: {impact_score:.1f}%")
print(f"üìä Categories Tested: {total_categories}")
print(f"‚úÖ Categories Passed: {successful_categories}")
print(f"‚ö†Ô∏è  Categories with Warnings: {len(warnings)}")
print(f"‚ùå Categories with Breaking Changes: {len(breaking_changes)}")

if impact_score >= 90:
    print("\nüéâ EXCELLENT: No significant cross-feature impact detected!")
    recommendation = "SAFE TO DEPLOY"
elif impact_score >= 75:
    print("\n‚úÖ GOOD: Minor issues detected but no breaking changes")
    recommendation = "SAFE WITH MONITORING"
elif impact_score >= 60:
    print("\n‚ö†Ô∏è  CAUTION: Some features may be affected")
    recommendation = "REVIEW ISSUES BEFORE DEPLOY"
else:
    print("\n‚ùå CRITICAL: Significant cross-feature impact detected")
    recommendation = "DO NOT DEPLOY - FIX ISSUES"

print(f"üìã Recommendation: {recommendation}")

# Export results
final_results = {
    'timestamp': datetime.now().isoformat(),
    'impact_analysis': results,
    'summary': {
        'impact_score': impact_score,
        'total_categories': total_categories,
        'successful_categories': successful_categories,
        'warnings': warnings,
        'breaking_changes': breaking_changes,
        'recommendation': recommendation
    }
}

with open('phase6_cross_feature_impact_results.json', 'w') as f:
    json.dump(final_results, f, indent=2, default=str)

print(f"\nüìÑ Detailed results saved to: phase6_cross_feature_impact_results.json")
print(f"Completed: {datetime.now().isoformat()}")
print("="*100)